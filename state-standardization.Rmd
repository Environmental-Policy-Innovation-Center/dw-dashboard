---
title: "SRF PPL & IUP Standardization"
author: "Walker Grimshaw & Phil Cork"
date: "2022-12-20"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

# Settings & Imports

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen=999999999)

library(tidyverse)
library(data.table)
library(janitor)
library(aws.s3)
library(googledrive)
library(googlesheets4)

```


# Import Clean State Data

## Year 0

```{r}

# Arkansas
source("year0/AR/code/clean-AR.R")
ar_clean_y0 <- clean_ar()

# Indiana
source("year0/IN/code/clean-IN.R")
in_clean_y0 <- clean_in()

# Michigan
source("year0/MI/code/clean-MI.R")
mi_clean_y0 <- clean_mi()

# Texas
source("year0/TX/code/clean-TX.R")
tx_clean_y0 <- clean_tx()

# Wisconsin
source("year0/WI/code/clean-WI.R")
wi_clean_y0 <- clean_wi()


```


## Year 1
```{r}
# Alabama
source("year1/AL/code/clean-AL.R")
al_clean_y1 <- clean_al()

# Alaska
source("year1/AK/code/clean-AK.R")
ak_clean_y1 <- clean_ak()

# Arkansas
source("year1/AR/code/clean-AR.R")
ar_clean_y1 <- clean_ar()

# Arizona
source("year1/AZ/code/clean-AZ.R")
az_clean_y1 <- clean_az()

# California
source("year1/CA/code/clean-CA.R")
ca_clean_y1 <- clean_ca()

# Colorado
source("year1/CO/code/clean-CO.R")
co_clean_y1 <- clean_co()

# Connecticut
source("year1/CT/code/clean-CT.R")
ct_clean_y1 <- clean_ct()

# Delaware
source("year1/DE/code/clean-DE.R")
de_clean_y1 <- clean_de()

# Florida
source("year1/FL/code/clean-FL.R")
fl_clean_y1 <- clean_fl()

# Georgia
source("year1/GA/code/clean-GA.R")
ga_clean_y1 <- clean_ga()

# Hawaii
source("year1/HI/code/clean-HI.R")
hi_clean_y1 <- clean_hi()

# Idaho
source("year1/ID/code/clean-ID.R")
id_clean_y1 <- clean_id()

# Illinois
source("year1/IL/code/clean-IL.R")
il_clean_y1 <- clean_il()

# Indiana
source("year1/IN/code/clean-IN.R")
in_clean_y1 <- clean_in()

# Iowa
source("year1/IA/code/clean-IA.R")
ia_clean_y1 <- clean_ia()

# Kansas
source("year1/KS/code/clean-KS.R")
ks_clean_y1 <- clean_ks()

# Kentucky
source("year1/KY/code/clean-KY.R")
ky_clean_y1 <- clean_ky()

# Louisiana
source("year1/LA/code/clean-LA.R")
la_clean_y1 <- clean_la()

# Maine
source("year1/ME/code/clean-ME.R")
me_clean_y1 <- clean_me()

# Maryland
source("year1/MD/code/clean-MD.R")
md_clean_y1 <- clean_md()

# Massachusetts
source("year1/MA/code/clean-MA.R")
ma_clean_y1 <- clean_ma()

# Michigan
source("year1/MI/code/clean-MI.R")
mi_clean_y1 <- clean_mi()

# Minnesota
source("year1/MN/code/clean-MN.R")
mn_clean_y1 <- clean_mn()

# Mississippi
source("year1/MS/code/clean-MS.R")
ms_clean_y1 <- clean_ms()

# Missouri
source("year1/MO/code/clean-MO.R")
mo_clean_y1 <- clean_mo()

# Montana
source("year1/MT/code/clean-MT.R")
mt_clean_y1 <- clean_mt()

# Nebraska
source("year1/NE/code/clean-NE.R")
ne_clean_y1 <- clean_ne()

# New Hampshire
source("year1/NH/code/clean-NH.R")
nh_clean_y1  <- clean_nh()

# New Jersey
source("year1/NJ/code/clean-NJ.R")
nj_clean_y1 <- clean_nj()

# New Mexico
source("year1/NM/code/clean-NM.R")
nm_clean_y1 <- clean_nm()

# Nevada
source("year1/NV/code/clean-NV.R")
nv_clean_y1 <- clean_nv()

# New York
source("year1/NY/code/clean-NY.R")
ny_clean_y1 <- clean_ny()

# North Carolina
source("year1/NC/code/clean-NC.R")
nc_clean_y1 <- clean_nc()

# North Dakota
source("year1/ND/code/clean-ND.R")
nd_clean_y1 <- clean_nd()

# Ohio
source("year1/OH/code/clean-OH.R")
oh_clean_y1 <- clean_oh()

# Oklahoma
source("year1/OK/code/clean-OK.R")
ok_clean_y1 <- clean_ok()

# Oregon
source("year1/OR/code/clean-OR.R")
or_clean_y1 <- clean_or()

# Pennsylvania
source("year1/PA/code/clean-PA.R")
pa_clean_y1 <- clean_pa()

# Rhode Island
source("year1/RI/code/clean-RI.R")
ri_clean_y1 <- clean_ri()

# South Carolina
source("year1/SC/code/clean-SC.R")
sc_clean_y1 <- clean_sc()

# South Dakota
source("year1/SD/code/clean-SD.R")
sd_clean_y1 <- clean_sd()

# Tennessee
source("year1/TN/code/clean-TN.R")
tn_clean_y1 <- clean_tn()

# Texas
source("year1/TX/code/clean-TX.R")
tx_clean_y1 <- clean_tx()

# Utah
source("year1/UT/code/clean-UT.R")
ut_clean_y1 <- clean_ut()

# Virginia
source("year1/VA/code/clean-VA.R")
va_clean_y1 <- clean_va()

# Vermont
source("year1/VT/code/clean-VT.R")
vt_clean_y1 <- clean_vt()

# Washington
source("year1/WA/code/clean-WA.R")
wa_clean_y1 <- clean_wa()

# West Virginia
source("year1/WV/code/clean-WV.R")
wv_clean_y1 <- clean_wv()

# Wisconsin
source("year1/WI/code/clean-WI.R")
wi_clean_y1 <- clean_wi()
  
```


# Post-Process

# Combine & Format States

```{r}
# create a list of all objects and boolean for whether each object is a dataframe
dfs <- sapply(.GlobalEnv, is.data.frame) 
# for each TRUE, add the dataframe rows to a single data.frame
combined_clean <- do.call(bind_rows, mget(names(dfs)[dfs]))

```

```{r}
# one-time export of Y0 + Y1 data of relevant states for 'apples-to-apples' mini-report
apples_to_apples <- combined_clean %>%
  filter(state %in% c("Arkansas", "Indiana", "Michigan", "Texas", "Wisconsin"))

write.csv(apples_to_apples, "apples-to-apples.csv", row.names = FALSE)

```


```{r}
 #one-time write of improved year 1 data for analysis
combined_clean <- combined_clean %>%
  filter(state_fiscal_year == "2023")

 # Writing to temp
 write.csv(combined_clean, file.path(tempdir(), "dwsrf-projects-sfy23.csv"), row.names = FALSE)

 #Putting in Bucket
 ##TODO: For all-y1-states PR, change this to remove the v1-1 suffix and change app to read the updated file
 put_object(
   file = file.path(tempdir(), "dwsrf-projects-sfy23.csv"),
   object = "clean_data/srf_project_priority_lists/dwsrf-projects-sfy23.csv",
   bucket = "water-team-data",
   acl = "public-read"
 )
```


```{r}

# Cleaning for table on web, currently leaves in Applicant projects for 1.1 release.
projects_clean_web <- combined_clean %>%
                          mutate(across(everything(), as.character),
                                 ) %>%
                          select(state, cities_served, borrower,
                                 pwsid, project_name, project_description,
                                 project_type, funding_amount, principal_forgiveness_amount, project_cost, requested_amount,
                                 population, disadvantaged, state_rank, state_score, funding_status, category) %>% 
                          rename(State = state,
                                Borrower = borrower, 
                                `City Served` = cities_served, 
                                `Project Description` = project_description,
                                `Project Type` = project_type,
                                `State Rank` = state_rank,
                                `State Score` = state_score,
                                `Project Cost` = project_cost,
                                `Requested Amount` = requested_amount,
                                `Funding Amount` = funding_amount, 
                                `Principal Forgiveness` = principal_forgiveness_amount,
                                `Meets State Disadvantaged Criteria` = disadvantaged,
                                 Population = population,
                                 PWSID = pwsid, 
                                `Project Name` = project_name,
                                `Funding Status` = funding_status,
                                `Category` = category)

```


# Add Data for Dashboard

```{r}

# SABs Data ## 
sabs_raw <- read_csv(get_object(object = "service_area_boundaries/sabs_app/Dev_Data_v1.csv", bucket = "tech-team-data"))

sabs_cleaned <- sabs_raw %>%
  mutate(Population = pop_tier1 + pop_tier2 + pop_tier3,
         Systems = num_sys_tier1 + num_sys_tier2 + num_sys_tier3) %>%
  rename(State = State_Name) %>%
  select(State, Population, Systems)

# Additional Data 
# TODO: Put this on AWS # 
# Sourced from Attachment A DWISNA Allotments and Pipe Estimates: https://www.epa.gov/system/files/documents/2023-04/Final_FY23%20DWSRF%20Allotment%20Memo%20and%20Attachments_April%202023.pdf
# Sourced from the 20-year need (page 9 + 10): https://www.epa.gov/system/files/documents/2023-04/Final_DWINSA%20Public%20Factsheet%204.4.23.pdf

URL <- "https://docs.google.com/spreadsheets/d/1hznoLfB8zMzs3jKfLjs-uy9R68mcFsYMAUg1gKXC17Y/edit#gid=0"

state_needs_estimates <- read_sheet(URL, sheet = "AdditionalData")

# Summary Data For Application # 
funded_states <- projects_clean_web %>%
  filter(`Funding Status` == "Funded") %>%
  mutate(Count = 1,
         DAC = as.numeric(ifelse(`Meets State Disadvantaged Criteria` == "Yes","1","0")),
         `Principal Forgiveness` = as.numeric(`Principal Forgiveness`),
         `Funding Amount` = as.numeric(`Funding Amount`)) %>%
  select(State, `Funding Amount`, `Principal Forgiveness`, DAC, Count) %>%
  group_by(State) %>%
  summarize_if(is.numeric,sum,na.rm = TRUE) %>%
  left_join(sabs_cleaned) %>%
  left_join(state_needs_estimates, by = c("State"= "State")) %>%
  mutate(FundPer100k = (`Funding Amount` / Population))

# separate states that only have applicant data to ensure they don't fall through the cracks while aggregating
only_applicant_states <- projects_clean_web %>%
  filter(Category == "2") %>%
  mutate(Count = 1,
         DAC = as.numeric(ifelse(`Meets State Disadvantaged Criteria` == "Yes","1","0")),
         `Principal Forgiveness` = as.numeric(`Principal Forgiveness`),
         `Funding Amount` = as.numeric(`Funding Amount`)) %>%
  select(State, `Funding Amount`, `Principal Forgiveness`, DAC, Count) %>%
  group_by(State) %>%
  summarize_if(is.numeric,sum,na.rm = TRUE) %>%
  left_join(sabs_cleaned) %>%
  left_join(state_needs_estimates, by = c("State"= "State")) %>%
  mutate(FundPer100k = (`Funding Amount` / Population))

state_category <- projects_clean_web %>%
  select(State, Category) %>%
  distinct()

# recombine states
all_states <- rbind(funded_states, only_applicant_states) %>%
  left_join(state_category)
                            
```


# Write to AWS

```{r}

## To put in an AWS bucket, you need these credentials
# Contact gabe@policyinnovation.org for questions/access
# !!! NOTE !!! NEVER WRITE CREDENTIALS IN CODE, ENTER IN CONSOLE AS ENVIRONMENT VARIABLES !!!! NOTE !!!! ### 

 # Sys.setenv("AWS_ACCESS_KEY_ID" = "",
 #             "AWS_SECRET_ACCESS_KEY" = "",
 #             "AWS_DEFAULT_REGION" = "us-east-1")


## Write State-Level Data ## 

# Writing to temp 
write.csv(all_states, file.path(tempdir(), "dw-dashboard-data_v1-1.csv"), row.names = FALSE)

#Putting in Bucket
##TODO: For all-y1-states PR, change this to remove the v1-1 suffix and change app to read the updated file
put_object(
  file = file.path(tempdir(), "dw-dashboard-data_v1-1.csv"),
  object = "apps/dw-dashboard/dw-dashboard-data_v1-1.csv",
  bucket = "water-team-data",
  acl = "public-read"
)

## Write Project-Level Data ###


# Writing to temp 
write.csv(projects_clean_web, file.path(tempdir(), "projects_clean_web.csv"), row.names=FALSE)

#Putting in Bucket
put_object(
  file = file.path(tempdir(), "projects_clean_web.csv"),
  object = "clean_data/srf_project_priority_lists/web_ppl_combined_clean.csv",
  bucket = "water-team-data",
  acl = "public-read"
)

# TODO: Push v[n] to srf_dashboard_data_public and update meta-data about when it was last updated

```

