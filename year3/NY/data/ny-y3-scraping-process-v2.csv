SCRAPING PROCESS FOR FUTURE REFERENCE

"This took quite a bit of time - chatGPT (even our upgraded account), only scraped fractions of a table at a time and started hallucinating columns and rows. I was also hitting some sort of token limit, such that chatGPT wasn't even scraping full tables when I passed it ~5 pages at a time. So, don't use chatGPT for this task. "
"Instead, use notebookLM - pass ~5 pages of tables at a time (any more than that will cause notebookLM to only digitize a fraction of the table), and add the 5 pages as a source to a project. Having the exact pages in the file name makes processing this ~55 page table easier. ONLY HAVE THIS SOURCE CHECKED, and ask notebookLM: ""digitize the tables in this PDF and return the information as text. there should be 8 columns."" - once the response processes (~a minute), CHECK the rows and columns to make sure you have all of the data. If 80% of the data was processed correctly, I copied & pasted the data to this spreadsheet, reorganized things if necessary, and then asked notebookLM to fix various columns: ""please fix the project and population columns. return a table with two columns as text"". If this didn't work, I fixed things manually or used the ""LEFT"" and ""RIGHT"" functions in googlesheets to extract necessary information. To make sure you have everything, I recommend greying out the top project of each page so you can compare with the original full PDF. "